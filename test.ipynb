{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cab7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f04c6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "S = 2\n",
    "D = 1\n",
    "E = 3\n",
    "C = 2\n",
    "d_ff = 6\n",
    "k = 2\n",
    "num_heads = 1\n",
    "d_head = int(D / num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b6014",
   "metadata": {},
   "source": [
    "## 1. Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f0c0c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1840],\n",
      "         [0.8077]]])\n",
      "torch.Size([1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "X_input = torch.rand(B, S, D) # (B=1, S=2, D=1)\n",
    "print(X_input)\n",
    "print(X_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60567c6d",
   "metadata": {},
   "source": [
    "## 2. Two-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ee937135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WQ1 (D=4, d_head=2)\n",
    "WQ1 = torch.rand(D, d_head)\n",
    "# WK1 (D=4, d_head=2)\n",
    "WK1 = torch.rand(D, d_head)\n",
    "# WV1 (D=4, d_head=2)\n",
    "WV1 = torch.rand(D, d_head)\n",
    "\n",
    "# WQ2 (D=4, d_head=2)\n",
    "WQ2 = torch.rand(D, d_head)\n",
    "# WK2 (D=4, d_head=2)\n",
    "WK2 = torch.rand(D, d_head)\n",
    "# WV2 (D=4, d_head=2)\n",
    "WV2 = torch.rand(D, d_head)\n",
    "\n",
    "WO = torch.rand(num_heads * d_head, D)\n",
    "\n",
    "# Q1 (B=2, S=3, d_head=2)\n",
    "Q1 = torch.matmul(X_input, WQ1)\n",
    "# K1 (B=2, S=3, d_head=2)\n",
    "K1 = torch.matmul(X_input, WK1)\n",
    "# V (B=2, S=3, d_head=2)\n",
    "V1 = torch.matmul(X_input, WV1)\n",
    "\n",
    "# Q1 (B=2, S=3, d_head=2)\n",
    "Q2 = torch.matmul(X_input, WQ2)\n",
    "# K1 (B=2, S=3, d_head=2)\n",
    "K2 = torch.matmul(X_input, WK2)\n",
    "# V1 (B=2, S=3, d_head=2)\n",
    "V2 = torch.matmul(X_input, WV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b20f1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B, S, d_head) -> (B, d_head, S)\n",
    "K1T = torch.transpose(K1, dim0=-2, dim1=-1)\n",
    "# (B, S, d_head) @ (B, d_head, S) = (B, S, S)\n",
    "QK1 = torch.matmul(Q1, K1T)\n",
    "QK1_norm = QK1 / torch.sqrt(torch.tensor(K1.size(-1)))\n",
    "# dim=-1 because we want to know for every query (a row),\n",
    "# how much attention it pays to every key (column).\n",
    "# As such, probabilities must sum to 1 across the columns.\n",
    "QK1_softmax = torch.softmax(QK1_norm, dim=-1)\n",
    "# (B, S, S) @ (B, S, d_head) = (B, S, d_head)\n",
    "attn1 = torch.matmul(QK1_softmax, V1)\n",
    "\n",
    "# (B, S, d_head) -> (B, d_head, S)\n",
    "K2T = torch.transpose(K2, dim0=-2, dim1=-1)\n",
    "# (B, S, d_head) @ (B, d_head, S) = (B, S, S)\n",
    "QK2 = torch.matmul(Q2, K2T)\n",
    "QK2_norm = QK2 / torch.sqrt(torch.tensor(K2.size(-1)))\n",
    "# dim=-1 because we want to know for every query (a row),\n",
    "# how much attention it pays to every key (column).\n",
    "# As such, probabilities must sum to 1 across the columns.\n",
    "QK2_softmax = torch.softmax(QK2_norm, dim=-1)\n",
    "# (B, S, S) @ (B, S, d_head) = (B, S, d_head)\n",
    "attn2 = torch.matmul(QK2_softmax, V2)\n",
    "\n",
    "# (B, S, d_head / num_heads) stacked on top (B, S, d_head / num_heads) = (B, S, d_head)\n",
    "attn_concat = torch.concat([attn1, attn2], dim=-1)\n",
    "\n",
    "# Why W_O? Every single feature in final attn_out is a weighted sum of all features from all heads.\n",
    "attn_out = torch.matmul(attn_concat, WO)\n",
    "\n",
    "# (B, S, D)\n",
    "x_attn = X_input + attn_out\n",
    "\n",
    "# Flatten x_attn (B * S, D)\n",
    "x_attn_flat = x_attn.view(-1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9aa6b3",
   "metadata": {},
   "source": [
    "## 3. Router Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b57ee375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, k, softmax_dim=-1):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=softmax_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # router_logits (B * S, D) @ (D, E) = (B * S, E)\n",
    "        router_logits = self.linear(x)\n",
    "        # p (B * S, E); along dim=expert\n",
    "        p = self.softmax(router_logits)\n",
    "\n",
    "        topk = torch.topk(p, k=self.k, dim=-1)\n",
    "        # expert_idx (B * S, k)\n",
    "        expert_idx = topk.indices\n",
    "        # gate_vals (B * S, k)\n",
    "        gate_vals = topk.values\n",
    "\n",
    "        return expert_idx, gate_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8bde18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "router = Router(input_dim=D, output_dim=E, k=2, softmax_dim=-1)\n",
    "# expert_idx (B * S, k); gate_vals (B * S, k)\n",
    "expert_idx, gate_vals = router(x_attn_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c69ee5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B = 1; S = 2 -> B * S = 2 | k = 2\n",
    "gate_vals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ffa73",
   "metadata": {},
   "source": [
    "## 4. Dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attn_expanded = torch.repeat_interleave(x_attn_flat, repeats=k, dim=0)\n",
    "# Flatten expert index along all tokens\n",
    "expert_idx_flat = expert_idx.view(-1)\n",
    "# Group expert indices together\n",
    "_, expert_idx_sort = torch.sort(expert_idx_flat, stable=True)\n",
    "# Permute x_attn so tokens go to their assigned expert\n",
    "x_attn_expGrouped = x_attn_expanded[expert_idx_sort]\n",
    "# Count # tokens / expert\n",
    "counts = torch.bincount(expert_idx_flat, minlength=E)\n",
    "# Split expanded tokens to their dedicated experts\n",
    "X_expert = torch.split(x_attn_expGrouped, split_size_or_sections=counts.tolist(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a2e0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 1))\n",
      "tensor([[0.8465],\n",
      "        [0.4763]])\n",
      "tensor([[0.8465],\n",
      "        [0.4763]])\n"
     ]
    }
   ],
   "source": [
    "for i, x_e in enumerate(X_expert):\n",
    "    print(x_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8f95af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, k, dim=-1):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        router_logits = self.linear(x)\n",
    "        p = self.softmax(router_logits)\n",
    "        topk = torch.topk(p, k=self.k, dim=-1)\n",
    "        expert_idx = topk.indices\n",
    "        gate_vals = topk.values\n",
    "\n",
    "        return expert_idx, gate_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "486bf3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 2],\n",
       "         [2, 0]]),\n",
       " tensor([[0.4621, 0.3809],\n",
       "         [0.4555, 0.3637]], grad_fn=<TopkBackward0>))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router = Router(input_dim=D, output_dim=E, k=2)\n",
    "router(x_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "88a73d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertFFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ExpertFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "experts = nn.ModuleList([\n",
    "            ExpertFFN(input_dim=D, hidden_dim=d_ff, output_dim=D)\n",
    "            for _ in range(E)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "ca91e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expertFFN_out = []\n",
    "for i, x_e in enumerate(X_expert):\n",
    "    out = experts[i](x_e)\n",
    "    expertFFN_out.append(out)\n",
    "y_expert = torch.cat(expertFFN_out, dim=0)\n",
    "\n",
    "# unsort_indices (B * S * k,)\n",
    "unsort_indices = torch.argsort(expert_idx_sort)\n",
    "# y_expert_restored (B * S * k, D)\n",
    "y_expert_restored = y_expert[unsort_indices]\n",
    "\n",
    "# y_expert_reshaped (B * S, k, D)\n",
    "y_expert_reshaped = torch.reshape(y_expert_restored, (B * S, k, D))\n",
    "# gate_vals_reshaped (B * S, k, 1)\n",
    "gate_vals_reshaped = torch.reshape(gate_vals, (B * S, k, 1))\n",
    "\n",
    "# weighted_expert_out (B * S, k, D)\n",
    "weighted_expert_out = y_expert_reshaped * gate_vals_reshaped\n",
    "y_tokens = torch.sum(weighted_expert_out, dim=1)\n",
    "y_tokens = torch.reshape(y_tokens, (B, S, D))\n",
    "\n",
    "# final_output (B, S, D)\n",
    "final_output = x_attn + y_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
